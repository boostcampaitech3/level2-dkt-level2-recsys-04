{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Tabular Models(DKT)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import Modules\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import os\n",
    "import random\n",
    "\n",
    "# Modeling\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "from sklearn.metrics import roc_auc_score\n",
    "from sklearn.metrics import accuracy_score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 파일 불러오기 \n",
    "\n",
    "data_dir = '/opt/ml/input/DKT/data'\n",
    "csv_file_path = os.path.join(data_dir,'train_data.csv')\n",
    "df = pd.read_csv(csv_file_path)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "5VZzei3DhfQy"
   },
   "source": [
    "### Train/Valid 데이터 전처리"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "def feature_engineering(df):\n",
    "    \n",
    "    #유저별 시퀀스를 고려하기 위해 아래와 같이 정렬\n",
    "    df.sort_values(by=['userID','Timestamp'], inplace=True)\n",
    "    \n",
    "    #유저들의 문제 풀이수, 정답 수, 정답률을 시간순으로 누적해서 계산\n",
    "    df['user_correct_answer'] = df.groupby('userID')['answerCode'].transform(lambda x: x.cumsum().shift(1))\n",
    "    df['user_total_answer'] = df.groupby('userID')['answerCode'].cumcount()\n",
    "    df['user_acc'] = df['user_correct_answer']/df['user_total_answer']\n",
    "\n",
    "    # => Null 값 생김\n",
    "    df = df.fillna(0)\n",
    "\n",
    "    # testId와 KnowledgeTag의 전체 정답률은 한번에 계산\n",
    "    # 아래 데이터는 제출용 데이터셋에 대해서도 재사용\n",
    "    correct_t = df.groupby(['testId'])['answerCode'].agg(['mean', 'sum'])\n",
    "    correct_t.columns = [\"test_mean\", 'test_sum']\n",
    "    correct_k = df.groupby(['KnowledgeTag'])['answerCode'].agg(['mean', 'sum'])\n",
    "    correct_k.columns = [\"tag_mean\", 'tag_sum']\n",
    "\n",
    "    df = pd.merge(df, correct_t, on=['testId'], how=\"left\")\n",
    "    df = pd.merge(df, correct_k, on=['KnowledgeTag'], how=\"left\")\n",
    "    \n",
    "    return df\n",
    "\n",
    "# train과 test 데이터셋은 사용자 별로 묶어서 분리를 해주어야함\n",
    "random.seed(42)\n",
    "def custom_train_test_split(df, ratio=0.7, split=True):\n",
    "    \n",
    "    users = list(zip(df['userID'].value_counts().index, df['userID'].value_counts()))\n",
    "    random.shuffle(users)\n",
    "    \n",
    "    max_train_data_len = ratio*len(df)\n",
    "    sum_of_train_data = 0\n",
    "    user_ids =[]\n",
    "\n",
    "    for user_id, count in users:\n",
    "        sum_of_train_data += count\n",
    "        if max_train_data_len < sum_of_train_data:\n",
    "            break\n",
    "        user_ids.append(user_id)\n",
    "\n",
    "\n",
    "    train = df[df['userID'].isin(user_ids)]\n",
    "    test = df[df['userID'].isin(user_ids) == False]\n",
    "\n",
    "    #test데이터셋은 각 유저의 마지막 interaction만 추출\n",
    "    test = test[test['userID'] != test['userID'].shift(-1)]\n",
    "    return train, test\n",
    "\n",
    "def scaling(df, num_cols, target):\n",
    "    # 사용할 변수만 남기기\n",
    "    df = df.drop(drop_cols, axis=1)\n",
    "\n",
    "    # 독립변수, 종속변수 나누기\n",
    "    y = df[target]\n",
    "    df = df.drop(target, axis=1)\n",
    "\n",
    "    # 수치형이랑 범주형 칼럼 나누기\n",
    "    X_num = df[num_cols]\n",
    "    X_cat = df.drop(num_cols, axis=1)\n",
    "\n",
    "    # 수치형 변수 스케일링 후 다시 합치기\n",
    "    scaler = StandardScaler()\n",
    "    scaler.fit(X_num)\n",
    "    X_scaled = scaler.transform(X_num)\n",
    "    X_scaled = pd.DataFrame(data=X_scaled, index=X_num.index, columns=X_num.columns)\n",
    "    X = pd.concat([X_scaled, X_cat], axis=1)\n",
    "\n",
    "    return X, y    \n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 사용할 수치형 변수, 카테고리 변수, target, 사용 안 할 변수\n",
    "num_cols = ['user_correct_answer', 'user_total_answer', \n",
    "         'user_acc', 'test_mean', 'test_sum', 'tag_mean','tag_sum','KnowledgeTag']\n",
    "cat_cols = []\n",
    "target = 'answerCode'\n",
    "use_cols = num_cols + cat_cols + [target]\n",
    "drop_cols = [x for x in df.columns if x not in use_cols]\n",
    "\n",
    "# feature engineering\n",
    "df = feature_engineering(df)\n",
    "\n",
    "# 카테고리 변수 더미변수화\n",
    "df = pd.get_dummies(df, columns=cat_cols ,drop_first=True)\n",
    "\n",
    "# 유저별 분리\n",
    "train, test = custom_train_test_split(df, ratio=0.7)\n",
    "\n",
    "# 사용 안 할 변수 drop하고 Train, Test 스케일링 후 X, y 값 분리\n",
    "X_train, y_train = scaling(train, num_cols=num_cols, target=target)\n",
    "X_test, y_test = scaling(test, num_cols=num_cols, target=target)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 사용할 모델들"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "def grid_searching(model, params, cv, is_grid=True): # grid search 하거나 안 하는 함수\n",
    "    model = model\n",
    "    if not is_grid: \n",
    "        model.fit(X_train, y_train)\n",
    "        predict = model.predict(X_test)\n",
    "        print(f\"ACC:{accuracy_score(y_test, predict)}, AUC:{roc_auc_score(y_test,predict)}\")\n",
    "\n",
    "    else:\n",
    "        grid_model = GridSearchCV(model, param_grid=params, cv=cv, refit=True)\n",
    "        grid_model.fit(X_train, y_train)\n",
    "        print('best parameters : ', grid_model.best_params_)\n",
    "        print('best score : ', grid_model.best_score_)\n",
    "        best_model = grid_model.best_estimator_\n",
    "        predict = best_model.predict(X_test)        \n",
    "        print(f\"ACC:{accuracy_score(y_test, predict)}, AUC:{roc_auc_score(y_test,predict)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Logistic Regression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_dir = '/opt/ml/input/DKT/data'\n",
    "\n",
    "own_df = pd.read_csv(os.path.join(data_dir, 'own_df.csv'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 사용할 수치형 변수, 카테고리 변수, target, 사용 안 할 변수\n",
    "num_cols = ['user_correct_answer', 'user_total_answer', \n",
    "         'user_acc', 'test_mean', 'test_sum', 'tag_mean','tag_sum','KnowledgeTag']\n",
    "cat_cols = []\n",
    "target = 'answerCode'\n",
    "use_cols = num_cols + cat_cols + [target]\n",
    "drop_cols = [x for x in df.columns if x not in use_cols]\n",
    "\n",
    "# 카테고리 변수 더미변수화\n",
    "df = pd.get_dummies(df, columns=cat_cols ,drop_first=True)\n",
    "\n",
    "# 유저별 분리\n",
    "train, test = custom_train_test_split(df, ratio=0.7)\n",
    "\n",
    "# 사용 안 할 변수 drop하고 Train, Test 스케일링 후 X, y 값 분리\n",
    "X_train, y_train = scaling(train, num_cols=num_cols, target=target)\n",
    "X_test, y_test = scaling(test, num_cols=num_cols, target=target)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ACC:0.5779770802192327, AUC:0.5864594514013118\n"
     ]
    }
   ],
   "source": [
    "from sklearn.linear_model import LogisticRegression\n",
    "\n",
    "model = LogisticRegression(C=0.01, penalty='l2', max_iter=500, solver='saga')\n",
    "params = {\n",
    "    'C': [0.001, 0.01, 0.1, 1, 10, 100],\n",
    "    'penalty': ['l1', 'l2']\n",
    "}\n",
    "grid_searching(model=model, params=params, cv=3, is_grid=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Decision Tree"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "best parameters :  {'max_depth': 2, 'min_samples_split': 2}\n",
      "best score :  0.7025370124142819\n",
      "ACC:0.5854509217737918, AUC:0.5928488372093024\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "DecisionTreeClassifier(max_depth=2)"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.tree import DecisionTreeClassifier\n",
    "\n",
    "model = DecisionTreeClassifier().fit(X_train, y_train)\n",
    "params = {\n",
    "    'max_depth': [2, 3],\n",
    "    'min_samples_split': [2, 3]\n",
    "}\n",
    "grid_searching(model=model, params=params, cv=3, is_grid=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Naive Bayes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ACC:0.6128550074738416, AUC:0.6174850924269529\n"
     ]
    }
   ],
   "source": [
    "from sklearn.naive_bayes import GaussianNB\n",
    "\n",
    "model = GaussianNB().fit(X_train, y_train)\n",
    "grid_searching(model=model, params=params, cv=3, is_grid=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### LDA"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "best parameters :  {'solver': 'svd'}\n",
      "best score :  0.710780836953719\n",
      "ACC:0.57847533632287, AUC:0.5866607036374478\n"
     ]
    }
   ],
   "source": [
    "from sklearn.discriminant_analysis import LinearDiscriminantAnalysis\n",
    "\n",
    "model = LinearDiscriminantAnalysis().fit(X_train, y_train)\n",
    "params = {\n",
    "    'solver': ['svd', 'lsqr', 'eigen']\n",
    "}\n",
    "grid_searching(model=model, params=params, cv=3, is_grid=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### QDA"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "best parameters :  {'reg_param': 0.1, 'store_covariance': True, 'tol': 0.01}\n",
      "best score :  0.7072849661162168\n",
      "ACC:0.5834578973592427, AUC:0.5909108527131783\n"
     ]
    }
   ],
   "source": [
    "from sklearn.discriminant_analysis import QuadraticDiscriminantAnalysis\n",
    "\n",
    "model = QuadraticDiscriminantAnalysis().fit(X_train, y_train)\n",
    "params = {\n",
    "    'reg_param': (0.01, 0.1), \n",
    "    'store_covariance': (True, False),\n",
    "    'tol': (0.01, 0.1), \n",
    "}\n",
    "grid_searching(model=model, params=params, cv=3, is_grid=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### SVM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.svm import SVC\n",
    "\n",
    "model = SVC(kernel='linear').fit(X_train, y_train)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Gaussian Process"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.gaussian_process import GaussianProcessClassifier\n",
    "\n",
    "model = GaussianProcessClassifier().fit(X_train, y_train)\n",
    "grid_searching(model=model, params=params, cv=3, is_grid=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### TabNet"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pytorch_tabnet.tab_model import TabNetClassifier\n",
    "\n",
    "model = TabNetClassifier().fit(X_train.values, y_train.values)\n",
    "grid_searching(model=model, params=params, cv=3, is_grid=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Random Forest"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.ensemble import RandomForestClassifier\n",
    "\n",
    "model = RandomForestClassifier().fit(X_train, y_train)\n",
    "grid_searching(model=model, params=params, cv=3, is_grid=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Extra Tree"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.ensemble import ExtraTreesClassifier\n",
    "\n",
    "model = ExtraTreesClassifier().fit(X_train, y_train)\n",
    "grid_searching(model=model, params=params, cv=3, is_grid=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### AdaBoost"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from sklearn.ensemble import AdaBoostClassifier\n",
    "\n",
    "model = AdaBoostClassifier(n_estimators=100).fit(X_train, y_train)\n",
    "params = {\n",
    "    'base_estimators': [DecisionTreeClassifier(max_depth=1), DecisionTreeClassifier(max_depth=2)],\n",
    "    'n_estimators' : [100, 200], \n",
    "    'learning_rate' : [0.05, 0.1]\n",
    "}\n",
    "grid_searching(model=model, params=params, cv=3, is_grid=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Gradient Boost"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.ensemble import GradientBoostingClassifier\n",
    "\n",
    "model = GradientBoostingClassifier(n_estimators=100).fit(X_train, y_train)\n",
    "params = {\n",
    "    'n_estimators' : [100, 200], \n",
    "    'learning_rate' : [0.05, 0.1]\n",
    "}\n",
    "grid_searching(model=model, params=params, cv=3, is_grid=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### XGBoost"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from xgboost import XGBClassifier\n",
    "\n",
    "model = XGBClassifier().fit(X_train, y_train)\n",
    "params = {\n",
    "    'n_estimators' : [100, 200], \n",
    "    'learning_rate' : [0.05, 0.1]\n",
    "}\n",
    "grid_searching(model=model, params=params, cv=3, is_grid=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### LGBM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/ml/input/DKT/.venv/lib/python3.8/site-packages/lightgbm/engine.py:177: UserWarning: Found `num_iterations` in params. Will use it instead of argument\n",
      "  _log_warning(f\"Found `{alias}` in params. Will use it instead of argument\")\n",
      "/opt/ml/input/DKT/.venv/lib/python3.8/site-packages/lightgbm/engine.py:177: UserWarning: Found `num_iterations` in params. Will use it instead of argument\n",
      "  _log_warning(f\"Found `{alias}` in params. Will use it instead of argument\")\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ACC:0.5829596412556054, AUC:0.5908228980322003\n"
     ]
    }
   ],
   "source": [
    "from lightgbm import LGBMClassifier\n",
    "\n",
    "\n",
    "model = LGBMClassifier(num_iterations=50).fit(X_train, y_train)\n",
    "param = {\n",
    "    'n_estimators' : [100, 200], \n",
    "    'learning_rate' : [0.05, 0.1]\n",
    "}\n",
    "grid_searching(model=model, params=params, cv=3, is_grid=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### CatBoost"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ACC:0.5864474339810662, AUC:0.5943843172331544\n"
     ]
    }
   ],
   "source": [
    "from catboost import CatBoostClassifier\n",
    "\n",
    "model = CatBoostClassifier(verbose=False).fit(X_train, y_train)\n",
    "grid_searching(model=model, params=params, cv=3, is_grid=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#### Voting Classifier\n",
    "\n",
    "# from sklearn.ensemble import VotingClassifier\n",
    "\n",
    "# model = VotingClassifier(\n",
    "#     estimators=[('lr', log_clf), ('rf', rnd_clf), ('svc', svm_clf)],\n",
    "#     voting='hard')\n"
   ]
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "00de636d1535660f3e06ba945348199ec7f3f3260a81554f1a2665fd5612deb4"
  },
  "kernelspec": {
   "display_name": "Python 3.8.5 ('.venv': venv)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
